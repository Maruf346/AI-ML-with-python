{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzhsUstQkXFKewPr8Yskjo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maruf346/AI-ML-with-python/blob/main/XOR_Classification_Using_Multilayer_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MLP from scratch**"
      ],
      "metadata": {
        "id": "-uAnX3JC6oWd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX8fuumTsChw",
        "outputId": "07cb32df-fe95-4b58-e5fa-56f49a7816da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.455025\n",
            "Epoch 100, Loss: 0.166641\n",
            "Epoch 200, Loss: 0.089031\n",
            "Epoch 300, Loss: 0.042722\n",
            "Epoch 400, Loss: 0.022757\n",
            "Epoch 500, Loss: 0.014000\n",
            "Epoch 600, Loss: 0.009617\n",
            "Epoch 700, Loss: 0.007132\n",
            "Epoch 800, Loss: 0.005580\n",
            "Epoch 900, Loss: 0.004538\n",
            "Epoch 1000, Loss: 0.003799\n",
            "Epoch 1100, Loss: 0.003251\n",
            "Epoch 1200, Loss: 0.002832\n",
            "Epoch 1300, Loss: 0.002502\n",
            "Epoch 1400, Loss: 0.002236\n",
            "Epoch 1500, Loss: 0.002018\n",
            "Epoch 1600, Loss: 0.001836\n",
            "Epoch 1700, Loss: 0.001682\n",
            "Epoch 1800, Loss: 0.001551\n",
            "Epoch 1900, Loss: 0.001437\n",
            "\n",
            "Predictions for XOR:\n",
            "[0 0] => 0\n",
            "[0 1] => 1\n",
            "[1 0] => 1\n",
            "[1 1] => 0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return 1 - np.tanh(x) ** 2\n",
        "\n",
        "class MLP:\n",
        "  def __init__(self, input_size, hidden_size, output_size, learning_rate = 0.1):\n",
        "    self.lr = learning_rate\n",
        "\n",
        "    #weight initialization\n",
        "    self.w1 = np.random.randn(input_size, hidden_size)\n",
        "    self.b1 = np.zeros((1, hidden_size))\n",
        "    self.w2 = np.random.randn(hidden_size, output_size)\n",
        "    self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.z1 = np.dot(x, self.w1) + self.b1\n",
        "    self.a1 = sigmoid(self.z1)\n",
        "    self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
        "    self.a2 = sigmoid(self.z2)\n",
        "\n",
        "    return self.a2\n",
        "\n",
        "  def backward(self, x, y, output):\n",
        "    m = x.shape[0]\n",
        "\n",
        "    dz2 = (output - y) * sigmoid_derivative(self.z2)\n",
        "    dw2 = np.dot(self.a1.T, dz2) / m\n",
        "    db2 = np.sum(dz2, axis = 0, keepdims = True) / m\n",
        "\n",
        "    dz1 = np.dot(dz2, self.w2.T) * sigmoid_derivative(self.z1)\n",
        "    dw1 = np.dot(x.T, dz1) / m\n",
        "    db1 = np.sum(dz1, axis = 0, keepdims = True) / m\n",
        "\n",
        "    self.w1 -= self.lr * dw1\n",
        "    self.b1 -= self.lr * db1\n",
        "    self.w2 -= self.lr * dw2\n",
        "    self.b2 -= self.lr * db2\n",
        "\n",
        "  def train(self, x, y, epochs):\n",
        "    for epoch in range(epochs):\n",
        "      output = self.forward(x)\n",
        "      self.backward(x, y, output)\n",
        "      if epoch % 100 == 0:\n",
        "        loss = np.mean((output - y) ** 2)\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
        "\n",
        "  def predict(self, x):\n",
        "    output = self.forward(x)\n",
        "    return np.round(output)\n",
        "\n",
        "# XOR dataset\n",
        "x = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "\n",
        "y = np.array([[0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0]])\n",
        "\n",
        "# Initialize and train MLP\n",
        "mlp = MLP(input_size=2, hidden_size=4, output_size=1, learning_rate=0.1)\n",
        "mlp.train(x, y, epochs=2000)\n",
        "\n",
        "# Test predictions\n",
        "predictions = mlp.predict(x)\n",
        "print(\"\\nPredictions for XOR:\")\n",
        "for i in range(len(x)):\n",
        "  print(f\"{x[i]} => {int(predictions[i][0])}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MLP using scikit-learn on manual dataset**"
      ],
      "metadata": {
        "id": "WAil5sJp62ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# manual dataset...\n",
        "X = np.array([\n",
        "    [0, 0, 0],\n",
        "    [0, 1, 1],\n",
        "    [1, 0, 1],\n",
        "    [1, 1, 0],\n",
        "    [1, 1, 1],\n",
        "    [0, 0, 1],\n",
        "    [1, 0, 0],\n",
        "    [0, 1, 0]\n",
        "])\n",
        "\n",
        "# Target: output = 1 if exactly two inputs are 1; else 0...\n",
        "y = np.array([0, 1, 1, 1, 0, 0, 0, 0])\n",
        "\n",
        "# Splitting... (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "#Modeling and traininggg...\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(4,), activation='tanh', max_iter=5000, random_state=1)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Prediction...\n",
        "train_preds = mlp.predict(X_train)\n",
        "test_preds = mlp.predict(X_test)\n",
        "\n",
        "print(\"Train Predictions:\")\n",
        "for i in range(len(X_train)):\n",
        "    print(f\"{X_train[i]} => {train_preds[i]} (Expected: {y_train[i]})\")\n",
        "\n",
        "print(\"\\nTest Predictions:\")\n",
        "for i in range(len(X_test)):\n",
        "    print(f\"{X_test[i]} => {test_preds[i]} (Expected: {y_test[i]})\")\n",
        "\n",
        "# Accuracy\n",
        "print(\"\\nTrain Accuracy:\", accuracy_score(y_train, train_preds))\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, test_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpj__-Hq6_CI",
        "outputId": "346eec5c-7568-41df-ae32-043e17f1bd07"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Predictions:\n",
            "[0 0 0] => 0 (Expected: 0)\n",
            "[0 1 0] => 0 (Expected: 0)\n",
            "[1 0 1] => 1 (Expected: 1)\n",
            "[1 1 1] => 0 (Expected: 0)\n",
            "[1 1 0] => 1 (Expected: 1)\n",
            "[1 0 0] => 0 (Expected: 0)\n",
            "\n",
            "Test Predictions:\n",
            "[0 1 1] => 0 (Expected: 1)\n",
            "[0 0 1] => 0 (Expected: 0)\n",
            "\n",
            "Train Accuracy: 1.0\n",
            "Test Accuracy: 0.5\n"
          ]
        }
      ]
    }
  ]
}