{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiU9HCrV7MvxI/TWHJTacc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maruf346/AI-ML-with-python/blob/main/Solution2_of_Lab_Final_Question2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSE 412 - ML Lab Final (Summer 2025)\n",
        "## Student Name: Maruf Hossain\n",
        "## ID: 221902318\n",
        "\n",
        "This notebook contains solutions for the ML Lab Final.  \n",
        "It is divided into 3 main parts:\n",
        "1. Titanic Dataset (Linear Regression)\n",
        "2. Liver Disease Dataset (KNN Classification)\n",
        "3. MNIST Dataset (CNN for Digit Recognition)\n"
      ],
      "metadata": {
        "id": "jkXvazC82PNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Titanic Dataset (titanic_train.csv)**\n",
        "### Steps:\n",
        "1. Load the Titanic dataset.\n",
        "2. Convert categorical features into numeric form using encoding.\n",
        "3. Normalize numerical features.\n",
        "4. Train a Linear Regression model (80/20 split).\n",
        "5. Print model accuracy on the test set.\n"
      ],
      "metadata": {
        "id": "hHfBrYB-2U6a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXnk17Uf1-fs"
      },
      "outputs": [],
      "source": [
        "# --- Titanic Dataset ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "url = \"https://tinyurl.com/labfinaldataset\"\n",
        "titanic = pd.read_csv(url + \"/titanic_train.csv\")\n",
        "\n",
        "# Drop irrelevant columns\n",
        "titanic = titanic.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
        "\n",
        "# Handle missing values\n",
        "titanic = titanic.fillna(method='ffill')\n",
        "\n",
        "# Encode categorical columns\n",
        "le = LabelEncoder()\n",
        "for col in titanic.select_dtypes(include=['object']).columns:\n",
        "    titanic[col] = le.fit_transform(titanic[col])\n",
        "\n",
        "# Features and target\n",
        "X = titanic.drop(\"Survived\", axis=1)\n",
        "y = titanic[\"Survived\"]\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Linear Regression\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions (rounding since regression outputs float)\n",
        "y_pred = np.round(model.predict(X_test))\n",
        "\n",
        "# Accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Titanic Linear Regression Accuracy:\", acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Liver Disease Dataset (ILPD.csv)**\n",
        "### Steps:\n",
        "1. Load ILPD dataset.\n",
        "2. Display first and last 10 rows.\n",
        "3. Preprocess dataset (scaling, encoding if needed).\n",
        "4. Train KNN classifier (K=5).\n",
        "5. Create two synthetic samples and test the model.\n"
      ],
      "metadata": {
        "id": "E2rYTvsW2ksT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Liver Disease Dataset ---\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load dataset\n",
        "ilpd = pd.read_csv(url + \"/ILPD.csv\")\n",
        "\n",
        "# Display first and last 10 rows\n",
        "print(\"First 10 rows:\\n\", ilpd.head(10))\n",
        "print(\"\\nLast 10 rows:\\n\", ilpd.tail(10))\n",
        "\n",
        "# Separate features and target\n",
        "X = ilpd.iloc[:, :-1]   # all except last column\n",
        "y = ilpd.iloc[:, -1]    # last column is target\n",
        "\n",
        "# Normalize features\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# KNN model (K=5)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Liver Disease KNN Accuracy:\", knn.score(X_test, y_test))\n",
        "\n",
        "# Create two synthetic samples (random)\n",
        "synthetic_samples = np.random.rand(2, X.shape[1])\n",
        "synthetic_samples = scaler.transform(synthetic_samples)\n",
        "\n",
        "predictions = knn.predict(synthetic_samples)\n",
        "print(\"Synthetic Sample Predictions:\", predictions)\n"
      ],
      "metadata": {
        "id": "47cFR47_2oyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. MNIST Handwritten Digit Classification (CNN)**\n",
        "### Steps:\n",
        "1. Load MNIST dataset from `tensorflow.keras.datasets`.\n",
        "2. Preprocess (normalize pixels 0â€“1).\n",
        "3. Visualize 5 random images with labels.\n",
        "4. Build CNN with 3 conv layers + pooling + dense layer.\n",
        "5. Train and evaluate with classification report + confusion matrix.\n"
      ],
      "metadata": {
        "id": "fETND0IU2ryz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MNIST CNN ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Reshape for CNN (28x28x1)\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Visualize 5 random images\n",
        "plt.figure(figsize=(10,4))\n",
        "for i in range(5):\n",
        "    idx = random.randint(0, len(x_train))\n",
        "    plt.subplot(1,5,i+1)\n",
        "    plt.imshow(x_train[idx].reshape(28,28), cmap=\"gray\")\n",
        "    plt.title(\"Label: \" + str(y_train[idx]))\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# CNN architecture\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "history = model.fit(x_train, y_train, epochs=3, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"MNIST CNN Test Accuracy:\", test_acc)\n",
        "\n",
        "# Predictions for classification report\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred_classes = y_pred.argmax(axis=1)\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_classes))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Myb1Am9B2u38"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}