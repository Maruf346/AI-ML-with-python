{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKl3QW0jjiunJTKTOL4i8Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maruf346/AI-ML-with-python/blob/main/Hidden_Markov_Model_(HMM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part-of-Speech Tagging**"
      ],
      "metadata": {
        "id": "wW0HjBJISRxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install NLTK\n",
        "!pip install nltk\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zievEjzTZYu",
        "outputId": "debecb65-5ba5-4b2f-c2e4-2cb9d5159f45"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation**"
      ],
      "metadata": {
        "id": "ucIfswA3SV1S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH_sCnHpR2F5",
        "outputId": "7b784242-63dd-4800-8d40-92649a8fd814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 49815\n",
            "Number of unique tags: 12\n"
          ]
        }
      ],
      "source": [
        "# Load the Brown corpus with universal tags\n",
        "tagged_sents = brown.tagged_sents(tagset='universal')\n",
        "\n",
        "# Prepare vocabulary and tags\n",
        "word_counts = defaultdict(int)\n",
        "tag_counts = defaultdict(int)\n",
        "\n",
        "# Count word and tag occurrences\n",
        "for sent in tagged_sents:\n",
        "    for word, tag in sent:\n",
        "        word_counts[word.lower()] += 1\n",
        "        tag_counts[tag] += 1\n",
        "\n",
        "# Create mappings for words and tags\n",
        "vocab = list(word_counts.keys())\n",
        "tags = list(tag_counts.keys())\n",
        "\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "tag2idx = {t: i for i, t in enumerate(tags)}\n",
        "\n",
        "# Print the number of unique words and tags\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Number of unique tags: {len(tags)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Initialization**"
      ],
      "metadata": {
        "id": "N-HYRAp-SfWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize HMM parameters\n",
        "n_states = len(tags)\n",
        "n_observations = len(vocab)\n",
        "\n",
        "# Uniform initial distribution (Start probabilities)\n",
        "start_prob = np.ones(n_states) / n_states\n",
        "\n",
        "# Initialize transition and emission matrices\n",
        "trans_mat = np.ones((n_states, n_states)) / n_states\n",
        "emit_mat = np.ones((n_states, n_observations)) / n_observations\n",
        "\n",
        "# Count occurrences for Maximum Likelihood Estimation (MLE)\n",
        "trans_counts = np.zeros((n_states, n_states))\n",
        "emit_counts = np.zeros((n_states, n_observations))\n"
      ],
      "metadata": {
        "id": "me7L-Z9rSiz6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parameter Estimation**"
      ],
      "metadata": {
        "id": "8EYp1l79SlfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate parameters from data\n",
        "for sent in tagged_sents:\n",
        "    prev_tag = None\n",
        "    for word, tag in sent:\n",
        "        word_idx = word2idx[word.lower()]\n",
        "        tag_idx = tag2idx[tag]\n",
        "\n",
        "        # Update start probabilities\n",
        "        if prev_tag is None:\n",
        "            start_prob[tag_idx] += 1\n",
        "        else:\n",
        "            trans_counts[prev_tag, tag_idx] += 1\n",
        "\n",
        "        # Update emission counts\n",
        "        emit_counts[tag_idx, word_idx] += 1\n",
        "\n",
        "        prev_tag = tag_idx\n",
        "\n",
        "# Normalize counts to probabilities\n",
        "start_prob = start_prob / start_prob.sum()\n",
        "trans_mat = trans_counts / trans_counts.sum(axis=1, keepdims=True)\n",
        "emit_mat = emit_counts / emit_counts.sum(axis=1, keepdims=True)\n"
      ],
      "metadata": {
        "id": "Zl2Yn848SqQN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Viterbi Implementation**"
      ],
      "metadata": {
        "id": "iDdCFKwRSsJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Viterbi decoding implementation\n",
        "def viterbi_decode(sentence, tags, start_prob, trans_mat, emit_mat):\n",
        "    # Convert sentence to indices\n",
        "    obs_seq = [word2idx.get(w.lower(), 0) for w in sentence.split()]\n",
        "    T = len(obs_seq)\n",
        "    N = len(tags)\n",
        "\n",
        "    # Initialize DP tables\n",
        "    delta = np.zeros((T, N))\n",
        "    psi = np.zeros((T, N), dtype=int)\n",
        "\n",
        "    # Initialization step\n",
        "    delta[0] = start_prob * emit_mat[:, obs_seq[0]]\n",
        "\n",
        "    # Recursion step\n",
        "    for t in range(1, T):\n",
        "        for j in range(N):\n",
        "            trans_probs = delta[t-1] * trans_mat[:, j]\n",
        "            psi[t, j] = np.argmax(trans_probs)\n",
        "            delta[t, j] = np.max(trans_probs) * emit_mat[j, obs_seq[t]]\n",
        "\n",
        "    # Backtracking to find the most probable state sequence\n",
        "    path = np.zeros(T, dtype=int)\n",
        "    path[-1] = np.argmax(delta[-1])\n",
        "    for t in range(T - 2, -1, -1):\n",
        "        path[t] = psi[t + 1, path[t + 1]]\n",
        "\n",
        "    return [tags[i] for i in path]\n"
      ],
      "metadata": {
        "id": "-onXfV-oSxO0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation Metrics**"
      ],
      "metadata": {
        "id": "WbO7Bw0aS0x7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the model on test data\n",
        "def evaluate_model(test_sents, word2idx, tag2idx, tags, start_prob, trans_mat, emit_mat):\n",
        "    y_true, y_pred = [], []\n",
        "    for sent in test_sents:\n",
        "        words = [w for w, t in sent]\n",
        "        true_tags = [t for w, t in sent]\n",
        "        pred_tags = viterbi_decode(' '.join(words), tags, start_prob, trans_mat, emit_mat)\n",
        "\n",
        "        y_true.extend(true_tags)\n",
        "        y_pred.extend(pred_tags)\n",
        "\n",
        "    # Calculate accuracy and generate classification report\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "yHuyk-btS2PC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing and Evaluation**"
      ],
      "metadata": {
        "id": "YZPP-1vmU_FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sents = brown.tagged_sents(categories='news', tagset='universal')[:100]\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(test_sents, word2idx, tag2idx, tags, start_prob, trans_mat, emit_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCVxas5LVCTp",
        "outputId": "f19ca850-4c8a-4d3e-f17c-831ce101a98b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.972663139329806\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           .       1.00      1.00      1.00       259\n",
            "         ADJ       0.90      0.95      0.92       120\n",
            "         ADP       0.98      0.95      0.96       256\n",
            "         ADV       0.91      0.90      0.90        58\n",
            "        CONJ       1.00      1.00      1.00        47\n",
            "         DET       0.99      1.00      0.99       279\n",
            "        NOUN       0.98      0.98      0.98       721\n",
            "         NUM       1.00      1.00      1.00        39\n",
            "        PRON       0.91      1.00      0.95        48\n",
            "         PRT       0.95      0.93      0.94        57\n",
            "        VERB       0.97      0.96      0.96       384\n",
            "\n",
            "    accuracy                           0.97      2268\n",
            "   macro avg       0.96      0.97      0.97      2268\n",
            "weighted avg       0.97      0.97      0.97      2268\n",
            "\n"
          ]
        }
      ]
    }
  ]
}